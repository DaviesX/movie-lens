{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Input,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.constraints import UnitNorm, NonNeg\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../movie-lens-small-latest-dataset/ratings.csv\")\n",
    "train_data = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "#train_data_mean = df.mean()\n",
    "#train_data_std = df.std()\n",
    "train_data['rating']  = (train_data['rating'] - 2.5)/2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model1():\n",
    "    \"\"\" Build a vanilla neural network predicting rating given user_id and movie_id.\n",
    "    \"\"\"\n",
    "    user = Input(shape=(1,))\n",
    "    movie = Input(shape=(1,))\n",
    "    # 611 = max_user_id + 1, 193610 = max_movie_id + 1\n",
    "    # TODO(summerxyt): Read csv and programatically get the number\n",
    "    user_embedding = Flatten()(\n",
    "        Embedding(611,32,  embeddings_regularizer=l2(0.01), name='user_embedding')(user))\n",
    "    movie_embedding = Flatten()(\n",
    "        Embedding(193610, 64, embeddings_regularizer=l2(0.01), name='movie_embedding')(movie))\n",
    "    x = tf.keras.layers.concatenate([user_embedding, movie_embedding])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    y = Dense(1, activation='tanh')(x)\n",
    "    model = Model(inputs=[user, movie], outputs=y)\n",
    "    model.compile(optimizer='adagrad', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def build_model2():\n",
    "    \"\"\" Build a model that the rating equals the dot multiplication of user embedding and moving\n",
    "           embedding.\n",
    "           \n",
    "           IMO, this is similar to matrix factoraization.\n",
    "    \"\"\"\n",
    "    user = Input(shape=(1,))\n",
    "    movie = Input(shape=(1,))\n",
    "    # TODO(summerxyt): It might be better to use embeddings_constraints. But tf and keras throws\n",
    "    # an error I couldn't figure out why.\n",
    "    user_embedding = Flatten()(\n",
    "        Embedding(611,15,embeddings_regularizer=l2(0.01),  name='user_embedding')(user))\n",
    "    movie_embedding = Flatten()(\n",
    "        Embedding(193610, 15,embeddings_regularizer=l2(0.01), name='movie_embedding')(movie))  \n",
    "    y = tf.keras.layers.Dot(axes=-1)([user_embedding, movie_embedding])\n",
    "\n",
    "    model = Model(inputs=[user, movie], outputs=y)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model1()\n",
    "\n",
    "history = model.fit(x=[train_data.userId.values, train_data.movieId.values],\n",
    "                    y=train_data.rating.values,\n",
    "                    batch_size=2048*32,\n",
    "                    #shuffle=True,\n",
    "                    epochs=1000,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = [tf.keras.callbacks.EarlyStopping(patience=2)],\n",
    "                    verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = model.get_layer('user_embedding').get_weights()[0]\n",
    "movie_embeddings = model.get_layer('movie_embedding').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieIds = df.movieId.unique()\n",
    "movieIds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embeddings_trunc = np.zeros((movieIds.shape[0],movie_embeddings.shape[1]))\n",
    "for new_row, movie_id in enumerate(movieIds):\n",
    "    movie_embeddings_trunc[new_row] = movie_embeddings[movie_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_clusters = KMeans(n_clusters=8).fit_predict(user_embeddings[1:])\n",
    "users_tsne = TSNE(n_components=2).fit_transform(user_embeddings[1:])\n",
    "plt.scatter(users_tsne[:,0], users_tsne[:,1], c=users_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_clusters = MiniBatchKMeans(n_clusters=8).fit_predict(movie_embeddings_trunc)\n",
    "#movies_clusters = KMeans(n_clusters=8, n_jobs=None).fit_predict(movie_embeddings_trunc)\n",
    "movies_tsne = TSNE(n_components=2).fit_transform(movie_embeddings_trunc)\n",
    "#just plot the first 1000 movies\n",
    "plt.scatter(movies_tsne[:1000,0], movies_tsne[:1000,1], c=movies_clusters[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('movies_clusters_trunc.txt', movies_clusters)\n",
    "#movie_clusters_trunc = np.load('movies_clusters_trunc.txt.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_clusters(movie_clusters_trunc, movieIds):\n",
    "    \n",
    "    def _get_movie_clusters(movie_clusters_trunc, movieIds):\n",
    "        mapping = {}\n",
    "        for new_row, movie_id in enumerate(movieIds):\n",
    "            mapping[movie_id] = movie_clusters_trunc[new_row]\n",
    "        return mapping\n",
    "    \n",
    "    movie_to_cluster = _get_movie_clusters(movie_clusters_trunc, movieIds)\n",
    "    \n",
    "    def _cluster_to_movie_id_list(movie_to_cluster):\n",
    "        d = {}\n",
    "        for movie_id, cluster_id in movie_to_cluster.items():\n",
    "            if cluster_id not in d:\n",
    "                d[cluster_id] = []\n",
    "            d[cluster_id].append(movie_id)\n",
    "        return d\n",
    "    \n",
    "    return _cluster_to_movie_id_list(movie_to_cluster)\n",
    "\n",
    "def get_readable_clusters(cluster_to_movie_id_list):\n",
    "    movies = pd.read_csv(\"../../movie-lens-small-latest-dataset/movies.csv\")\n",
    "    \n",
    "    d = {}\n",
    "    for cluster, movie_id_list in cluster_to_movie_id_list.items():\n",
    "        d[cluster] = movies[movies['movieId'].isin(movie_id_list)]\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = get_movie_clusters(movies_clusters, movieIds)\n",
    "readable_clusters = get_readable_clusters(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_for_toy_story = None\n",
    "for c, l in clusters.items():\n",
    "    if 1 in l:\n",
    "        cluster_for_toy_story = c\n",
    "        break\n",
    "readable_clusters[cluster_for_toy_story]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_ids, movie_ids, model):\n",
    "    return (model.predict([user_ids, movie_ids]) + 1 ) * 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = predict_rating([1,1,1],[1,2,3], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
